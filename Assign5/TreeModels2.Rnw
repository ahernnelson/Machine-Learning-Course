%%%%%%%%%%%%%%%%%%%%%
%% start of header %%
%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}

\usepackage{graphics,latexsym,geometry,amsmath,makeidx,fancyhdr}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{color} 
\usepackage{setspace}
\usepackage{biblatex}

\addbibresource{references.bib}

\pagestyle{fancy}

\doublespacing
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}
%% you should change the following: title, date, author
\title{Machine Learing with Tree-Based Models in R Part 2}
\date{Spring 2018}
\author{Ahern Nelson}

%%%%%%%%%%%%%%%%%%%%
%%% end of header %%
%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%
%%% start document %%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%This prints the header/title:
\maketitle
\section{Bagged Trees}
We have discussed the problems present in decision trees one of those being the high
variance pressent in them. For example a small chang in data can result a drastically 
different decision boundaries making model inerpretation somewhat precarious. In this 
section we introduce bagged trees avaerages many trees to reduce variance. This process of 
combining several models inot one is wha tis known as an ensemble model. Bagging not only 
recues varaiance, but also helps overfitting the data. Bagging is short for bootsrap 
aggregation. It uses bootstrapping which is the process sammpling rows from the dataset 
with replacment and then wee aggreates this via averages. \\

Bagging works by drawing B samples from the orignal training set. 


\printbibliography

%% end the entire document - this should always be the last line!
\end{document}

%%%%%%%%%%%%%%%%%%%%%%
%%%% end document %%%%
%%%%%%%%%%%%%%%%%%%%%%
